{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "from importlib import reload\n",
    "from sys import modules\n",
    "from schema import TYPE_SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(modules[\"schema\"])\n",
    "spec = TYPE_SPEC[\"mnist_graph\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [()]                      0         \n",
      "                                                                 \n",
      " graph_update_1 (GraphUpdate  ()                       1184      \n",
      " )                                                               \n",
      "                                                                 \n",
      " readout_1 (Readout)         (784, 32)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (784, 120)                3960      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (784, 10)                 1210      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,354\n",
      "Trainable params: 6,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = tf.keras.layers.Input(type_spec=spec)\n",
    "gnn = tfgnn.keras.ConvGNNBuilder(\n",
    "    lambda edge: tfgnn.keras.layers.SimpleConvolution(\n",
    "        tf.keras.layers.Dense(32)\n",
    "    ),\n",
    "    lambda node: tfgnn.keras.layers.NextStateFromConcat(\n",
    "        tf.keras.layers.Dense(32),\n",
    "    ),\n",
    "    )\n",
    "hidden = gnn.Convolve()(input)\n",
    "# hidden = tfgnn.keras.layers.Pool(\n",
    "#     tag=tfgnn.SOURCE,\n",
    "#     edge_set_name=\"connected\",\n",
    "#     reduce_type=\"sum\"\n",
    "# )(hidden)\n",
    "hidden = tfgnn.keras.layers.Readout(node_set_name=\"pixel\")(hidden)\n",
    "hidden = tf.keras.layers.Dense(120, activation=\"tanh\")(hidden)\n",
    "output = tf.keras.layers.Dense(10, activation=\"softmax\")(hidden)\n",
    "\n",
    "model = tf.keras.Model(input, output, name = \"gnn\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generators import load_dataset_from_data\n",
    "reload(modules[\"generators\"])\n",
    "\n",
    "(trn_img, trn_lbl), _ = tf.keras.datasets.mnist.load_data()\n",
    "trn_img = trn_img/255\n",
    "trn_graph = load_dataset_from_data(trn_img, trn_lbl, 32, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 21:28:52.081571: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "print(type(trn_graph))\n",
    "print(len(list(trn_graph.take(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trn_graph, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6cc1b48ae16f10eafcfc5d9834160e52b9d26f3378f7461b2bc28961673c20a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
